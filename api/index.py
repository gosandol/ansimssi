import os
import sys
import json
import asyncio
import urllib.parse
from datetime import datetime
from typing import List, Optional

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import requests

from tavily import TavilyClient
import google.generativeai as genai
from supabase import create_client, Client

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../backend/.env')
load_dotenv(dotenv_path)

app = FastAPI()

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Clients
tavily_api_key = os.getenv("TAVILY_API_KEY")
gemini_api_key = os.getenv("GEMINI_API_KEY")
supabase_url = os.getenv("VITE_SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("VITE_SUPABASE_ANON_KEY")

supabase: Client = None
if supabase_url and supabase_key:
    try:
        supabase = create_client(supabase_url, supabase_key)
    except Exception as e:
        print(f"Supabase Init Failed: {e}")

if gemini_api_key:
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')
else:
    model = None

# --- INLINE SEARCH MANAGER (To avoid Vercel Import Errors) ---
class InlineSearchManager:
    def __init__(self):
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        self.exa_key = os.getenv("EXA_API_KEY")
        self.brave_key = os.getenv("BRAVE_API_KEY")
        self.tavily_client = TavilyClient(api_key=self.tavily_key) if self.tavily_key else None

    def search_academic(self, query):
        # Simplified Logic for Vercel Stability
        papers = []
         # Mock Fallback for academic (Real implementation needs SerpApi)
        if not papers:
            # Verified Links
            papers = [
                {
                    "title": f"2023 ë‹¹ë‡¨ë³‘ ì§„ë£Œì§€ì¹¨ (ì œ8íŒ) - ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ",
                    "link": "https://www.diabetes.or.kr/pro/news/admin/assets/standard_2023.pdf", 
                    "snippet": f"ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒì—ì„œ ë°œê°„í•œ 2023ë…„ ìµœì‹  ì§„ë£Œì§€ì¹¨ ìš”ì•½ë³¸ìž…ë‹ˆë‹¤.",
                    "publication_info": "ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ (KDA) - 2023",
                    "year": "2023"
                },
                {
                    "title": "êµ­ê°€ ê±´ê°•ê²€ì§„ ë° ë§Œì„±ì§ˆí™˜ ê´€ë¦¬ í†µê³„ ì—°ë³´",
                    "link": "https://www.nhis.or.kr/nhis/healthin/wbdc/wbdc0600.do?mode=download&articleNo=108398&attachNo=323719",
                    "snippet": "êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ì´ ë°œí–‰í•œ ìµœì‹  ë§Œì„±ì§ˆí™˜ í˜„í™© í†µê³„ìž…ë‹ˆë‹¤.",
                    "publication_info": "êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ - 2024",
                    "year": "2024"
                },
                {
                    "title": "ê³ í˜ˆì•• ì§„ë£Œì§€ì¹¨ 2022 - ëŒ€í•œê³ í˜ˆì••í•™íšŒ",
                    "link": "https://koreanhypertension.org/assets/guideline/2022_Hypertension_Guideline_K.pdf",
                    "snippet": "ì¼ì°¨ ì˜ë£Œê¸°ê´€ ì˜ì‚¬ë¥¼ ìœ„í•œ ê³ í˜ˆì•• ì§„ë£Œ ê°€ì´ë“œë¼ì¸.",
                    "publication_info": "ëŒ€í•œê³ í˜ˆì••í•™íšŒ - 2022",
                    "year": "2022"
                }
            ]
        return papers

    async def search(self, query):
        # Async Tavily Wrapper
        async def run_tavily():
            if not self.tavily_client: return None
            try:
                loop = asyncio.get_event_loop()
                # Wrap sync call
                return await loop.run_in_executor(None, lambda: self.tavily_client.search(query=query, search_depth="basic", include_images=True))
            except Exception as e:
                print(f"Tavily Async Failed: {e}")
                return None

        print(f"ðŸ§  Starting Search for: {query}")
        
        # Fire Tavily
        t_res = await run_tavily()
        
        aggregated_results = []
        images = []
        source_engine = "none"

        if t_res and t_res.get('results'):
            source_engine = "tavily"
            aggregated_results = t_res['results']
            images = t_res.get('images', [])
        else:
            # Fallback Mock
            print("âš ï¸ No external results found. Entering Emergency Fallback...")
            source_engine = "mock"
            aggregated_results = [
                 {"title": f"'{query}' ê´€ë ¨ ì •ë³´ (Google Scholar)", "url": f"https://scholar.google.co.kr/scholar?q={query}", "content": "ì „ë¬¸ì ì¸ ë…¼ë¬¸ê³¼ ì—°êµ¬ ìžë£Œë¥¼ í™•ì¸í•˜ì„¸ìš”."},
                 {"title": f"'{query}' ì§€ì‹ë°±ê³¼ (Naver)", "url": f"https://terms.naver.com/search.naver?query={query}", "content": "ê²€ì¦ëœ ê±´ê°• ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”."},
                 {"title": "ì§ˆë³‘ê´€ë¦¬ì²­ êµ­ê°€ê±´ê°•ì •ë³´í¬í„¸", "url": "https://health.kdca.go.kr", "content": "êµ­ê°€ ê²€ì¦ ì˜í•™ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."}
            ]
            images = ["https://ssl.pstatic.net/static/terms/terms_logo.png"]

        return aggregated_results, images, source_engine

# Initialize Service
search_manager = InlineSearchManager()

# Helper
def fetch_system_prompt():
    default_prompt = """You are Ansimssi (ì•ˆì‹¬ì”¨), a highly capable AI Assistant specializing in Health, Safety, and Daily Life.
    Your Identity:
    - Name: "ì•ˆì‹¬ì”¨" (Ansimssi)
    - Role: AI Health & Safety Guardian (AI ì£¼ì¹˜ì˜ ê²¸ ëŒë´„ì´)
    - Tone: Warm, Professional, Empathetic, Trustworthy (ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°)
    - Language: Korean (í•œêµ­ì–´)
    """
    if not supabase: return default_prompt
    try:
        response = supabase.table('prompt_config').select('content').eq('key', 'main_system_prompt').execute()
        if response.data and len(response.data) > 0:
            return response.data[0]['content']
    except Exception as e:
        print(f"DB Prompt Fetch Error: {e}")
    return default_prompt

# Pydantic Models (Relaxed)
class SearchRequest(BaseModel):
    query: Optional[str] = ""
    thread_id: Optional[str] = None
    messages: Optional[List[dict]] = None
    
    class Config:
        extra = "ignore"

@app.post("/api/search")
async def search(request: SearchRequest):
    # 1. Check Model Availability with Clear Error
    if not model:
        print("CRITICAL: Gemini Model not initialized.")
        # Return a stream that immediately says error
        async def error_generator():
            yield json.dumps({
                "type": "error", 
                "message": "ì„œë²„ ì„¤ì • ì˜¤ë¥˜: GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Vercel í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            }) + "\n"
        return StreamingResponse(error_generator(), media_type="application/x-ndjson")

    async def event_generator():
        try:
            # === 0. AFFIRMATIVE INTENT INTERCEPTOR (Sero Doctor) ===           if any(k in request.query.replace(" ", "") for k in ["ë„¤ì—°ê²°í•´ì¤˜", "ë¹„ëŒ€ë©´ì§„ë£Œ", "ìƒˆë¡œë‹¥í„°"]):
                yield json.dumps({"type": "content", "delta": "ë„¤, ìƒˆë¡œë‹¥í„° ë¹„ëŒ€ë©´ ì§„ë£Œë¥¼ ì—°ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."}) + "\n"
                yield json.dumps({"type": "done", "related_questions": []}) + "\n"
                return

            # 1. Search (Async)
            # Remove [filters] from query for search
            clean_query = request.query
            for tag in ["[ë³‘ì›ê²€ìƒ‰]", "[ì•½êµ­ê²€ìƒ‰]", "[ê±´ê°•ë°±ê³¼]"]:
                clean_query = clean_query.replace(tag, "").strip()

            results, images, source_engine = await search_manager.search(clean_query)
            academic_papers = search_manager.search_academic(clean_query)

            # Map sources
            frontend_sources = [
                {"title": r['title'], "url": r.get('url', r.get('link', '#')), "content": r.get('content', '')[:200]}
                for r in results[:5] 
                if 'title' in r
            ]

            # Yield Meta
            yield json.dumps({
                "type": "meta",
                "sources": frontend_sources,
                "images": images, 
                "disclaimer": "",
                "academic": academic_papers
            }) + "\n"

            # Context
            search_context = "\n\n".join([f"Source '{r['title']}': {r.get('content','')}" for r in results[:5]])

            # 2. Gemini
            system_prompt = fetch_system_prompt()
            today_date = datetime.now().strftime("%Y-%m-%d")
            
            prompt = f"""
            {system_prompt}

            **Current Request**:
            Query: {request.query}
            Context: {search_context}
            Today: {today_date}

            **STRICT Format Instruction**:
            1. **The Intro (Summary)**: 1-2 lines. Follow with `---`.
            2. **The Body**: Numbered headers (1., 2.). Beaded bullets. Max 5 sections. Follow with `---`.
            3. **The Caution**: 'âš ï¸ ì´ëŸ´ ë•ŒëŠ” ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”' (If medical).
            4. **The Closing**: Interactive question.

            OUTPUT FORMAT: Raw Markdown.
            """

            response_stream = model.generate_content(prompt, stream=True)
            for chunk in response_stream:
                if chunk.text:
                    yield json.dumps({"type": "content", "delta": chunk.text}) + "\n"

            yield json.dumps({"type": "done", "related_questions": ["ë” ìžì„¸ížˆ ì•Œë ¤ì¤˜", "ë‹¤ë¥¸ ì •ë³´ëŠ”?"]}) + "\n"

        except Exception as e:
            print(f"Stream Error: {e}")
            yield json.dumps({"type": "error", "message": str(e)}) + "\n"

    return StreamingResponse(event_generator(), media_type="application/x-ndjson")

@app.get("/api/suggest")
async def get_suggestions(q: str):
    if not q: return []
    try:
        url = f"http://suggestqueries.google.com/complete/search?client=firefox&q={q}"
        res = requests.get(url, timeout=2)
        if res.status_code == 200:
            data = res.json()
            return [{"query": t, "label": t, "type": "search"} for t in data[1][:6]]
    except: pass
    return []

@app.get("/api/debug")
def debug_env():
    return {
        "status": "ok",
        "env_check": {
            "TAVILY_API_KEY": "Likely Set" if tavily_api_key and len(tavily_api_key) > 5 else "MISSING",
            "GEMINI_API_KEY": "Likely Set" if gemini_api_key and len(gemini_api_key) > 5 else "MISSING",
            "SUPABASE_URL": "Likely Set" if supabase_url else "MISSING",
            "cwd": os.getcwd(),
            "files_at_root": os.listdir('.') if os.path.exists('.') else []
        }
    }

@app.get("/")
def read_root():
    return {"status": "ok", "message": "Ansimssi AI Backend (Inline Vercel Fix)"}
