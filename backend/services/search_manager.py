import os
from tavily import TavilyClient
from .knowledge_base import KnowledgeBase

class SearchManager:
    def __init__(self):
        # API Keys
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        self.exa_key = os.getenv("EXA_API_KEY")
        self.brave_key = os.getenv("BRAVE_API_KEY")
        
        # Clients
        self.tavily_client = TavilyClient(api_key=self.tavily_key) if self.tavily_key else None
        self.knowledge_base = KnowledgeBase()

    def search_academic(self, query):
        """
        Search for academic papers using SerpApi (Google Scholar)
        """
        papers = []
        
        if self.serpapi_key:
            try:
                print(f"Attempting Academic Search (Scholar) for: {query}")
                import requests
                params = {
                    "engine": "google_scholar",
                    "q": query + " filetype:pdf",  # Prefer PDFs
                    "api_key": self.serpapi_key,
                    "num": 6,
                    "hl": "ko",
                    "as_ylo": "2020" # Recent papers (since 2020)
                }
                response = requests.get("https://serpapi.com/search", params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    organic_results = data.get("organic_results", [])
                    
                    papers = []
                    for item in organic_results:
                        # Logic to find the BEST link (PDF preferred)
                        # Scholar results often have 'resources': [{'title': 'so and so', 'link': '...pdf'}]
                        best_link = item.get("link")
                        pdf_link = None
                        
                        resources = item.get("resources", [])
                        for res in resources:
                            if "file_format" in res and "pdf" in res["file_format"].lower():
                                pdf_link = res.get("link")
                                break
                            if res.get("link", "").lower().endswith(".pdf"):
                                pdf_link = res.get("link")
                                break
                        
                        # If we found a direct PDF link, use it!
                        final_link = pdf_link if pdf_link else best_link
                        
                        # Extract Year cleanly
                        pub_info = item.get("publication_info", {}).get("summary", "")
                        year = ""
                        import re
                        match = re.search(r'\b20\d{2}\b', pub_info)
                        if match:
                            year = match.group(0)

                        papers.append({
                            "title": item.get("title"),
                            "link": final_link,
                            "snippet": item.get("snippet", ""),
                            "publication_info": pub_info,
                            "year": year
                        })
            except Exception as e:
                print(f"Academic Search Failed: {e}")
                
        # Mock Fallback if no results
        if not papers:
            print("Using Mock Academic Data")
            papers = [
                {
                    "title": f"A Study on {query} and its Clinical Implications",
                    "link": "#",
                    "snippet": f"This paper discusses the recent advancements in treating {query}...",
                    "publication_info": "Nature Medicine - 2025",
                    "year": "2025"
                },
                {
                    "title": f"Analysis of {query} in Korean Population",
                    "link": "#",
                    "snippet": "Comprehensive survey data regarding patient outcomes...",
                    "publication_info": "Journal of Korean Medical Science - 2024",
                    "year": "2024"
                }
            ]
            
        return papers

    async def search(self, query):
        """
        Execute Parallel Race Strategy (The "Gemini" Speed)
        """
        import asyncio
        
        results = []
        images = []
        source_engine = "none"

        # Define async wrappers for each provider
        async def run_google():
            if not self.serpapi_key: return None
            try:
                # SerpApi is blocking, so run in executor
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_google_sync, query)
            except Exception as e:
                print(f"Google Async Failed: {e}")
                return None

        async def run_tavily():
            if not self.tavily_client: return None
            try:
                # Tavily client is blocking
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_tavily_sync, query)
            except Exception as e:
                print(f"Tavily Async Failed: {e}")
                return None

        async def run_exa():
            if not self.exa_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_exa_sync, query)
            except Exception as e:
                print(f"Exa Async Failed: {e}")
                return None
                
        async def run_brave():
            if not self.brave_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_brave_sync, query)
            except Exception as e:
                print(f"Brave Async Failed: {e}")
                return None

        # --- THE GREAT AGGREGATION (GEMINI STYLE) ---
        print(f"ğŸ§  Starting Deep Research (Aggregation) for: {query}")
        
        # Fire all requests simultaneously
        tasks = [
            asyncio.create_task(run_tavily()), # General Web & News
            asyncio.create_task(run_google()), # Real-time Sync & Local
            asyncio.create_task(run_exa()),    # Deep Content match
            # asyncio.create_task(run_brave())   # Backup (Skip to save quota/time if others sufficient)
        ]
        
        # Wait for ALL to complete (Enrichment Strategy)
        # We accept a slightly higher latency (e.g. max 4s) for significantly better quality.
        done, pending = await asyncio.wait(tasks, timeout=4.5) 
        
        aggregated_results = []
        seen_urls = set()
        
        # Collect results from all successful engines
        for task in done:
            try:
                res = task.result()
                if res and res.get('results'):
                    engine_name = res.get('engine')
                    print(f"âœ… {engine_name} contributed {len(res['results'])} results.")
                    
                    # Add images if available
                    if res.get('images'):
                        images.extend(res['images'])
                        
                    # Add unique results
                    for item in res['results']:
                        url = item.get('url')
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            # Tag the source engine for debugging/quality check
                            item['source_engine'] = engine_name 
                            aggregated_results.append(item)
            except Exception as e:
                print(f"Task Error during aggregation: {e}")
                
        # Cancel any stragglers
        for t in pending: t.cancel()

        # Tier 5: Emergency Fallback if ABSOLUTELY nothing found
        if not aggregated_results:
            print("âš ï¸ No external results found. Entering Emergency Fallback...")
            kb_match = self.knowledge_base.find_match(query)
            if kb_match:
                print("Tier 5-A Success: KB")
                aggregated_results = kb_match.get('sources', [])
                images = kb_match.get('images', [])
                source_engine = "knowledge_base"
            else:
                print("Tier 5-B: Hardcoded Mock")
                aggregated_results, images = self._get_mock_data(query)
                source_engine = "mock"
        else:
            source_engine = "hybrid_aggregation"
            
        # Limit total context to avoid token overflow? 
        # For now, let's keep top 8-10 high quality ones.
        # Simple heuristic: Interleave results? 
        # Or just take top 10 from the mixed bag.
        final_results = aggregated_results[:12]
        
        print(f"ğŸ† Final Aggregated Context: {len(final_results)} items.")

        return final_results, images, source_engine

    # --- Sync Helper Implementations ---
    def _search_google_sync(self, query):
        print(f"Attempting Tier 1 (Google) for: {query}")
        import requests
        params = {
            "engine": "google",
            "q": query,
            "api_key": self.serpapi_key,
            "num": 5,
            "hl": "ko", "gl": "kr"
        }
        response = requests.get("https://serpapi.com/search", params=params)
        if response.status_code == 200:
            data = response.json()
            organic = data.get("organic_results", [])
            results = [{"title": i.get("title"), "url": i.get("link"), "content": i.get("snippet", "")} for i in organic]
            if results: return {"engine": "google", "results": results, "images": []}
        return None

    def _search_tavily_sync(self, query):
        print(f"Attempting Tier 2 (Tavily) for: {query}")
        search_result = self.tavily_client.search(query=query, search_depth="basic", include_images=True)
        results = search_result.get("results", [])
        images = search_result.get("images", [])
        if results: return {"engine": "tavily", "results": results, "images": images}
        return None

    def _search_exa_sync(self, query):
        print(f"Attempting Tier 3 (Exa) for: {query}")
        import requests
        headers = {"accept": "application/json", "content-type": "application/json", "x-api-key": self.exa_key}
        response = requests.post("https://api.exa.ai/search", json={"query": query, "numResults": 5, "useAutoprompt": True, "contents": {"text": True}}, headers=headers)
        if response.status_code == 200:
            data = response.json()
            results = [{"title": i.get("title") or "Exa Result", "url": i.get("url"), "content": i.get("text", "")[:300] + "..."} for i in data.get("results", [])]
            if results: return {"engine": "exa", "results": results, "images": []}
        return None

    def _search_brave_sync(self, query):
         print(f"Attempting Tier 4 (Brave) for: {query}")
         import requests
         headers = {"Accept": "application/json", "X-Subscription-Token": self.brave_key}
         response = requests.get("https://api.search.brave.com/res/v1/web/search", params={"q": query, "count": 5}, headers=headers)
         if response.status_code == 200:
             data = response.json()
             results = [{"title": i.get("title"), "url": i.get("url"), "content": i.get("description")} for i in data.get("web", {}).get("results", [])]
             if results: return {"engine": "brave", "results": results, "images": []}
         return None

    def _get_mock_data(self, query):
        """
        Hardcoded mock data for core scenarios (copied from previous main.py logic)
        """
        results = []
        images = []
        
        q_lower = query.lower()
        
        if "ë³´ê±´ì†Œ" in query or "health center" in q_lower:
            results = [
                {"title": "ë³´ê±´ì†Œ ì´ìš©ì•ˆë‚´ - G-Health ê³µê³µë³´ê±´í¬í„¸", "url": "https://www.g-health.kr/portal/index.do", "content": "ì „êµ­ ë³´ê±´ì†Œ ì°¾ê¸° ë° ì§„ë£Œ ì‹œê°„ ì•ˆë‚´. ë‚´ê³¼, ì¹˜ê³¼, í•œë°© ì§„ë£Œ ë“± ë³´ê±´ì†Œì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."},
                {"title": "ë³´ê±´ì†Œ - ì°¾ê¸°/ì•ˆë‚´/ì˜ˆì•½ - ì„œìš¸íŠ¹ë³„ì‹œ", "url": "https://health.seoul.go.kr", "content": "ì„œìš¸ì‹œ ë‚´ 25ê°œ ìì¹˜êµ¬ ë³´ê±´ì†Œ ìœ„ì¹˜ ë° ì—°ë½ì²˜ ì •ë³´. ì˜ˆë°©ì ‘ì¢…, ëŒ€ì‚¬ì¦í›„êµ° ê´€ë¦¬ ë“± ì‹œë¯¼ ê±´ê°• ì„œë¹„ìŠ¤ ì•ˆë‚´."},
                {"title": "ë™ë„¤ ì˜ì›ê³¼ ë³´ê±´ì†Œ, ë¬´ì—‡ì´ ë‹¤ë¥¼ê¹Œ? - í—¬ìŠ¤ì¡°ì„ ", "url": "https://m.health.chosun.com", "content": "ë³´ê±´ì†ŒëŠ” êµ­ê°€ì—ì„œ ìš´ì˜í•˜ëŠ” ê³µê³µ ì˜ë£Œê¸°ê´€ìœ¼ë¡œ, ì¼ë°˜ ë³‘ì˜ì›ë³´ë‹¤ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ì§„ë£Œ ë° ì˜ˆë°©ì ‘ì¢…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."},
                {"title": "ë³´ê±´ì†Œ ëª¨ë°”ì¼ í—¬ìŠ¤ì¼€ì–´ - í•œêµ­ê±´ê°•ì¦ì§„ê°œë°œì›", "url": "https://www.khealth.or.kr", "content": "ìŠ¤ë§ˆíŠ¸í°ì„ í™œìš©í•œ ë§ì¶¤í˜• ê±´ê°•ê´€ë¦¬ ì„œë¹„ìŠ¤. ë³´ê±´ì†Œ ì „ë¬¸ê°€ê°€ ë¹„ëŒ€ë©´ìœ¼ë¡œ ê±´ê°•ìƒë‹´ ë° ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."}
            ]
            images = [
                "https://news.seoul.go.kr/welfare/files/2020/02/602ff579e0a01.jpg",
                "https://www.korea.kr/newsWeb/resources/attaches/2021.05/20/094e9f735870ad46b412953258849646.jpg",
                "https://t1.daumcdn.net/cfile/tistory/99857B3359D8878D32",
                "https://www.yongin.go.kr/resources/images/hist/content/img_hist_2020_04_01.jpg"
            ]

        elif "ë‹¹ë‡¨" in query or "diabetes" in q_lower:
             results = [
                {"title": "ë‹¹ë‡¨ë³‘ì˜ ì¦ìƒê³¼ ì§„ë‹¨ - ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ", "url": "https://www.diabetes.or.kr", "content": "ëŒ€í‘œì ì¸ ì¦ìƒì€ ë‹¤ë‡¨, ë‹¤ìŒ, ë‹¤ì‹ì…ë‹ˆë‹¤. ì´ìœ  ì—†ëŠ” ì²´ì¤‘ ê°ì†Œë‚˜ í”¼ë¡œê°ë„ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤."},
                {"title": "ë‹¹ë‡¨ë³‘ ì´ˆê¸°ì¦ìƒ 5ê°€ì§€ - ì„œìš¸ì•„ì‚°ë³‘ì›", "url": "https://www.amc.seoul.kr", "content": "1. ì¦ì€ ì†Œë³€ 2. ì‹¬í•œ ê°ˆì¦ 3. ë°°ê³ í”” 4. ì²´ì¤‘ ê°ì†Œ 5. ì‹œì•¼ íë¦¼. ì¡°ê¸° ë°œê²¬ì´ í•©ë³‘ì¦ ì˜ˆë°©ì˜ í•µì‹¬ì…ë‹ˆë‹¤."},
                {"title": "ë‹¹ë‡¨ë³‘ ê´€ë¦¬ ê°€ì´ë“œ - ì§ˆë³‘ê´€ë¦¬ì²­", "url": "https://health.kdca.go.kr", "content": "ì•½ë¬¼ ìš”ë²• ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ì´ìš”ë²•ê³¼ ìš´ë™ìš”ë²•ì´ ë³‘í–‰ë˜ì–´ì•¼ í˜ˆë‹¹ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
             ]
             images = [
                 "https://www.amc.seoul.kr/asan/images/healthinfo/disease/disease_img_01.jpg",
                 "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000123&fileSn=1",
                 "https://i.ytimg.com/vi/example_diabetes/maxresdefault.jpg",
                 "https://post-phinf.pstatic.net/MjAyMTEyMTZfMjQ5/MDAxNjM5NjM4ODQ5MjQ5.example.jpg"
             ]
        
        elif "ê³ í˜ˆì••" in query or "hypertension" in q_lower:
            results = [
                {"title": "ê³ í˜ˆì••ì˜ ì§„ë‹¨ê³¼ ì¹˜ë£Œ - ì§ˆë³‘ê´€ë¦¬ì²­ êµ­ê°€ê±´ê°•ì •ë³´í¬í„¸", "url": "https://health.kdca.go.kr", "content": "ê³ í˜ˆì••ì€ ì¹¨ë¬µì˜ ì‚´ì¸ìë¡œ ë¶ˆë¦¬ë©°, ë‡Œì¡¸ì¤‘ ë° ì‹¬í˜ˆê´€ ì§ˆí™˜ì˜ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. ì •ê¸°ì ì¸ í˜ˆì•• ì¸¡ì •ê³¼ ìƒí™œ ìŠµê´€ ê°œì„ ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤."},
                {"title": "ëŒ€í•œê³ í˜ˆì••í•™íšŒ - ì¼ë°˜ì¸/í™˜ìë¥¼ ìœ„í•œ ì •ë³´", "url": "https://www.koreanhypertension.org", "content": "ì˜¬ë°”ë¥¸ í˜ˆì•• ì¸¡ì •ë²•, ê³ í˜ˆì•• ì•½ë¬¼ ë³µìš© ê°€ì´ë“œ, ì‹ë‹¨ ê´€ë¦¬ ë“± ê³ í˜ˆì•• í™˜ìë¥¼ ìœ„í•œ ì „ë¬¸ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"title": "ê³ í˜ˆì•• ë‚®ì¶”ëŠ” ë°©ë²• 5ê°€ì§€ - ì‚¼ì„±ì„œìš¸ë³‘ì› ê±´ê°•ì¹¼ëŸ¼", "url": "http://www.samsunghospital.com", "content": "1. ì²´ì¤‘ ê°ëŸ‰ 2. ì‹ë‹¨ ì¡°ì ˆ(ì €ì—¼ì‹) 3. ê·œì¹™ì ì¸ ìš´ë™ 4. ê¸ˆì—° 5. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬"},
                {"title": "ê³ í˜ˆì••, ì•½ í‰ìƒ ë¨¹ì–´ì•¼ í•˜ë‚˜ìš”?", "url": "https://www.hidoc.co.kr", "content": "ê³ í˜ˆì•• ì•½ì€ í•œ ë²ˆ ë¨¹ìœ¼ë©´ í‰ìƒ ë¨¹ì–´ì•¼ í•œë‹¤ëŠ” ì˜¤í•´ì™€ ì§„ì‹¤. ìƒí™œ ìŠµê´€ ê°œì„ ìœ¼ë¡œ í˜ˆì••ì´ ì¡°ì ˆë˜ë©´ ì•½ì„ ì¤„ì´ê±°ë‚˜ ëŠì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
            ]
            images = [
                "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000345&fileSn=1", 
                "https://i.ytimg.com/vi/Ofg98y0d_E4/maxresdefault.jpg", 
                "https://post-phinf.pstatic.net/MjAyMTAzMTZfMTQy/MDAxNjE1ODczMzE4MjU4.Kj-YlWfWlM_Zz3yW.jpg",
                "http://www.samsunghospital.com/upload/editor/20200518_1.jpg"
            ]
        
        elif "ê°ê¸°" in query or "cold" in q_lower:
            results = [
                 {"title": "ê°ê¸°ì™€ ë…ê°ì˜ ì°¨ì´ì  - ì§ˆë³‘ê´€ë¦¬ì²­", "url": "https://kdca.go.kr", "content": "ê°ê¸°ëŠ” ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì— ì˜í•œ ìƒê¸°ë„ ê°ì—¼ì´ë©°, ë…ê°ì€ ì¸í”Œë£¨ì—”ì ë°”ì´ëŸ¬ìŠ¤ì— ì˜í•œ ê¸‰ì„± í˜¸í¡ê¸° ì§ˆí™˜ì…ë‹ˆë‹¤."},
                 {"title": "ê°ê¸° ë¹¨ë¦¬ ë‚«ëŠ” ë²• 10ê°€ì§€", "url": "https://www.healthline.com", "content": "ì¶©ë¶„í•œ ìˆ˜ë¶„ ì„­ì·¨, íœ´ì‹, ê°€ìŠµê¸° ì‚¬ìš©, ë”°ëœ»í•œ ì°¨ ë§ˆì‹œê¸° ë“±ì´ ë„ì›€ì´ ë©ë‹ˆë‹¤."},
                 {"title": "ì•½ ë¨¹ì–´ë„ ê°ê¸°ê°€ ì•ˆ ë‚«ëŠ” ì´ìœ ", "url": "https://www.youtube.com/watch?v=example", "content": "ê°ê¸°ì•½ì€ ì¦ìƒì„ ì™„í™”í•  ë¿ ë°”ì´ëŸ¬ìŠ¤ë¥¼ ì¹˜ë£Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©´ì—­ë ¥ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."}
            ]
            images = [
                "https://img.freepik.com/free-photo/sick-woman-blowing-her-nose_23-2147743128.jpg",
                "https://i.ytimg.com/vi/example/maxresdefault.jpg",
                "https://images.unsplash.com/photo-1513201099718-4ed89549448f?ixlib=rb-1.2.1&auto=format&fit=crop&w=1000&q=80",
                "https://post-phinf.pstatic.net/MjAxOTEwMjlfMjQ4/MDAxNTcyMzE1MjE4MjQ4.example.jpg"
            ]

        return results, images
