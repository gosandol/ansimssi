import os
from tavily import TavilyClient
from .knowledge_base import KnowledgeBase

class SearchManager:
    def __init__(self):
        # API Keys
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        self.exa_key = os.getenv("EXA_API_KEY")
        self.brave_key = os.getenv("BRAVE_API_KEY")
        
        # Clients
        self.tavily_client = TavilyClient(api_key=self.tavily_key) if self.tavily_key else None
        self.knowledge_base = KnowledgeBase()

    def search_academic(self, query):
        """
        Intelligent Academic Search with Source Routing
        Decides between Google Scholar (papers) vs Google Search (Gov/Hospital PDFs) based on intent.
        """
        papers = []
        
        # 1. Intent Classification
        q_lower = query.lower()
        
        # Policy / Statistics -> Government Sources
        gov_keywords = ["í†µê³„", "í˜„í™©", "ì •ì±…", "ê°€ì´ë“œë¼ì¸", "ì§€ì¹¨", "ë²•ë ¹", "ë³´ê±´ì†Œ", "ì§ˆë³‘ê´€ë¦¬ì²­", "stats", "policy", "guideline"]
        is_gov = any(k in q_lower for k in gov_keywords)
        
        # Clinical / Patient Info -> Major Hospitals
        clinical_keywords = ["ì¦ìƒ", "ì¹˜ë£Œë²•", "ìˆ˜ìˆ ", "ì‹ì´ìš”ë²•", "ì¢‹ì€ ìŒì‹", "í”¼í•´ì•¼", "symptom", "treatment", "died"]
        is_clinical = any(k in q_lower for k in clinical_keywords)
        
        target_engine = "google_scholar"
        search_query = query
        
        if is_gov:
            print(f"ğŸ›ï¸ Routing to Government Sources for: {query}")
            target_engine = "google"
            # Prioritize credible KR gov sites
            search_query = f"{query} site:go.kr OR site:or.kr filetype:pdf"
            
        elif is_clinical:
            print(f"ğŸ¥ Routing to Medical Institutions for: {query}")
            target_engine = "google"
            # Major KR Hospitals & Health Agencies
            search_query = f"{query} site:snuh.org OR site:amc.seoul.kr OR site:samsunghospital.com OR site:kdca.go.kr filetype:pdf"
        
        else:
            print(f"ğŸ“ Routing to Academic Scholar for: {query}")
            search_query = query + " filetype:pdf"

        if self.serpapi_key:
            try:
                import requests
                
                if target_engine == "google_scholar":
                    params = {
                        "engine": "google_scholar",
                        "q": search_query,
                        "api_key": self.serpapi_key,
                        "num": 6,
                        "hl": "ko",
                        "as_ylo": "2020"
                    }
                else: # target_engine == "google" (custom filtered)
                    params = {
                        "engine": "google",
                        "q": search_query,
                        "api_key": self.serpapi_key,
                        "num": 6,
                        "hl": "ko",
                        "gl": "kr"
                    }
                    
                response = requests.get("https://serpapi.com/search", params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if target_engine == "google_scholar":
                        raw_results = data.get("organic_results", [])
                    else:
                        raw_results = data.get("organic_results", [])
                        
                    for item in raw_results:
                        # Common parsing logic
                        title = item.get("title")
                        link = item.get("link")
                        snippet = item.get("snippet", "")
                        
                        # Special handling for Scholar resources
                        if target_engine == "google_scholar":
                            resources = item.get("resources", [])
                            for res in resources:
                                if res.get("link", "").lower().endswith(".pdf"):
                                    link = res.get("link")
                                    break
                        
                        # For Standard Google, link is usually direct, but check snippet for date
                        pub_info = item.get("publication_info", {}).get("summary", "") # Scholar only
                        if not pub_info:
                            # Try to parse source/date from snippet or displayed link
                            source = item.get("displayed_link", "Source")
                            pub_info = f"{source}"

                        # Extract Year
                        import re
                        year = ""
                        match = re.search(r'\b20\d{2}\b', snippet + pub_info)
                        if match:
                            year = match.group(0)

                        papers.append({
                            "title": title,
                            "link": link,
                            "snippet": snippet,
                            "publication_info": pub_info,
                            "year": year
                        })
                        
            except Exception as e:
                print(f"Academic/Source Search Failed: {e}")
                
        # Mock Fallback if no results
        if not papers:
            print("Using Mock Academic Data")
            print("Using Mock Academic Data (Korean Optimized - Verified Links)")
            # Fallback data with REAL viewable PDF links for demonstration
            # Updated 2026-01-09 with Verified URLs
            papers = [
                {
                    "title": f"2023 ë‹¹ë‡¨ë³‘ ì§„ë£Œì§€ì¹¨ (ì œ8íŒ) - ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ",
                    "link": "https://www.diabetes.or.kr/pro/news/admin/assets/standard_2023.pdf", # Direct PDF
                    "snippet": f"ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒì—ì„œ ë°œê°„í•œ 2023ë…„ ìµœì‹  ì§„ë£Œì§€ì¹¨ ìš”ì•½ë³¸ì…ë‹ˆë‹¤. í•œêµ­ì¸ í™˜ìì— ìµœì í™”ëœ ì•½ë¬¼ ì¹˜ë£Œ ë° ìƒí™œ ìŠµê´€ ê°€ì´ë“œë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                    "publication_info": "ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ (KDA) - 2023",
                    "year": "2023"
                },
                {
                    "title": "êµ­ê°€ ê±´ê°•ê²€ì§„ ë° ë§Œì„±ì§ˆí™˜ ê´€ë¦¬ í†µê³„ ì—°ë³´",
                    "link": "https://www.nhis.or.kr/nhis/healthin/wbdc/wbdc0600.do?mode=download&articleNo=108398&attachNo=323719", # NHIS valid download
                    "snippet": "êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ì´ ë°œí–‰í•œ ìµœì‹  ë§Œì„±ì§ˆí™˜ í˜„í™© í†µê³„ì…ë‹ˆë‹¤. ê³ í˜ˆì••, ë‹¹ë‡¨ë³‘ ìœ ë³‘ë¥  ë° ê´€ë¦¬ ì‹¤íƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "publication_info": "êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨ - 2024",
                    "year": "2024"
                },
                {
                    "title": "ê³ í˜ˆì•• ì§„ë£Œì§€ì¹¨ 2022 - ëŒ€í•œê³ í˜ˆì••í•™íšŒ",
                    "link": "https://koreanhypertension.org/assets/guideline/2022_Hypertension_Guideline_K.pdf", # Reliable Society Link
                    "snippet": "ì¼ì°¨ ì˜ë£Œê¸°ê´€ ì˜ì‚¬ë¥¼ ìœ„í•œ ê³ í˜ˆì•• ì§„ë£Œ ê°€ì´ë“œë¼ì¸. ì§„ë‹¨ ê¸°ì¤€ ë° ëª©í‘œ í˜ˆì•• ì„¤ì •ì— ëŒ€í•œ ê·¼ê±° ì¤‘ì‹¬ì˜ ê¶Œê³ ì•ˆì…ë‹ˆë‹¤.",
                    "publication_info": "ëŒ€í•œê³ í˜ˆì••í•™íšŒ - 2022",
                    "year": "2022"
                }
            ]
            
        return papers

    async def search(self, query):
        """
        Execute Parallel Race Strategy (The "Gemini" Speed)
        """
        import asyncio
        
        results = []
        images = []
        source_engine = "none"

        # Define async wrappers for each provider
        async def run_google():
            if not self.serpapi_key: return None
            try:
                # SerpApi is blocking, so run in executor
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_google_sync, query)
            except Exception as e:
                print(f"Google Async Failed: {e}")
                return None

        async def run_tavily():
            if not self.tavily_client: return None
            try:
                # Tavily client is blocking
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_tavily_sync, query)
            except Exception as e:
                print(f"Tavily Async Failed: {e}")
                return None

        async def run_exa():
            if not self.exa_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_exa_sync, query)
            except Exception as e:
                print(f"Exa Async Failed: {e}")
                return None
                
        async def run_brave():
            if not self.brave_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_brave_sync, query)
            except Exception as e:
                print(f"Brave Async Failed: {e}")
                return None

        # --- THE GREAT AGGREGATION (GEMINI STYLE) ---
        print(f"ğŸ§  Starting Deep Research (Aggregation) for: {query}")
        
        # Fire all requests simultaneously
        tasks = [
            asyncio.create_task(run_tavily()), # General Web & News
            asyncio.create_task(run_google()), # Real-time Sync & Local
            asyncio.create_task(run_exa()),    # Deep Content match
            # asyncio.create_task(run_brave())   # Backup (Skip to save quota/time if others sufficient)
        ]
        
        # Wait for ALL to complete (Enrichment Strategy)
        # VOICE-FIRST OPTIMIZATION: Adaptive Latency
        # 1. Primary Wait: 2.0s (Acceptable voice delay)
        done, pending = await asyncio.wait(tasks, timeout=2.0)
        
        aggregated_results = []
        seen_urls = set()
        
        # Check yield from first wave
        initial_yield = 0
        for task in done:
            try:
                res = task.result()
                if res and res.get('results'):
                    initial_yield += len(res['results'])
            except: pass
            
        # 2. Decision Gate: Do we have enough?
        # If we have < 4 results, it's too thin. Pay the latency cost for intelligence.
        # If we have >= 4, SPEED WINS.
        if initial_yield < 4 and len(pending) > 0:
            print(f"âš ï¸ Low yield ({initial_yield}) after 2s. Extending wait for deep research...")
            second_wave_done, second_wave_pending = await asyncio.wait(pending, timeout=2.0)
            
            # Meritge second wave into done
            done = done.union(second_wave_done)
            pending = second_wave_pending # Remainder are truly slow/dead
        else:
            print(f"âš¡ï¸ Voice Speed Success: {initial_yield} results in <2.0s. Proceeding.")
        
        # Collect results from all successful engines (merged from both waves)
        for task in done:
            try:
                res = task.result()
                if res and res.get('results'):
                    engine_name = res.get('engine')
                    print(f"âœ… {engine_name} contributed {len(res['results'])} results.")
                    
                    # Add images if available
                    if res.get('images'):
                        images.extend(res['images'])
                        
                    # Add unique results
                    for item in res['results']:
                        url = item.get('url')
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            # Tag the source engine for debugging/quality check
                            item['source_engine'] = engine_name 
                            aggregated_results.append(item)
            except Exception as e:
                print(f"Task Error during aggregation: {e}")
                
        # Cancel any stragglers (Too slow for voice)
        for t in pending: t.cancel()

        # Tier 5: Emergency Fallback if ABSOLUTELY nothing found
        if not aggregated_results:
            print("âš ï¸ No external results found. Entering Emergency Fallback...")
            kb_match = self.knowledge_base.find_match(query)
            if kb_match:
                print("Tier 5-A Success: KB")
                aggregated_results = kb_match.get('sources', [])
                images = kb_match.get('images', [])
                source_engine = "knowledge_base"
            else:
                print("Tier 5-B: Hardcoded Mock")
                aggregated_results, images = self._get_mock_data(query)
                source_engine = "mock"
        else:
            source_engine = "hybrid_aggregation"
            
        # Limit total context to avoid token overflow? 
        # For now, let's keep top 8-10 high quality ones.
        # Simple heuristic: Interleave results? 
        # Or just take top 10 from the mixed bag.
        final_results = aggregated_results[:12]
        
        print(f"ğŸ† Final Aggregated Context: {len(final_results)} items.")

        return final_results, images, source_engine

    # --- Sync Helper Implementations ---
    def _search_google_sync(self, query):
        print(f"Attempting Tier 1 (Google) for: {query}")
        import requests
        params = {
            "engine": "google",
            "q": query,
            "api_key": self.serpapi_key,
            "num": 5,
            "hl": "ko", "gl": "kr"
        }
        response = requests.get("https://serpapi.com/search", params=params)
        if response.status_code == 200:
            data = response.json()
            organic = data.get("organic_results", [])
            results = [{"title": i.get("title"), "url": i.get("link"), "content": i.get("snippet", "")} for i in organic]
            if results: return {"engine": "google", "results": results, "images": []}
        return None

    def _search_tavily_sync(self, query):
        print(f"Attempting Tier 2 (Tavily) for: {query}")
        search_result = self.tavily_client.search(query=query, search_depth="basic", include_images=True)
        results = search_result.get("results", [])
        images = search_result.get("images", [])
        if results: return {"engine": "tavily", "results": results, "images": images}
        return None

    def _search_exa_sync(self, query):
        print(f"Attempting Tier 3 (Exa) for: {query}")
        import requests
        headers = {"accept": "application/json", "content-type": "application/json", "x-api-key": self.exa_key}
        response = requests.post("https://api.exa.ai/search", json={"query": query, "numResults": 5, "useAutoprompt": True, "contents": {"text": True}}, headers=headers)
        if response.status_code == 200:
            data = response.json()
            results = [{"title": i.get("title") or "Exa Result", "url": i.get("url"), "content": i.get("text", "")[:300] + "..."} for i in data.get("results", [])]
            if results: return {"engine": "exa", "results": results, "images": []}
        return None

    def _search_brave_sync(self, query):
         print(f"Attempting Tier 4 (Brave) for: {query}")
         import requests
         headers = {"Accept": "application/json", "X-Subscription-Token": self.brave_key}
         response = requests.get("https://api.search.brave.com/res/v1/web/search", params={"q": query, "count": 5}, headers=headers)
         if response.status_code == 200:
             data = response.json()
             results = [{"title": i.get("title"), "url": i.get("url"), "content": i.get("description")} for i in data.get("web", {}).get("results", [])]
             if results: return {"engine": "brave", "results": results, "images": []}
         return None

    def _get_mock_data(self, query):
        """
        Hardcoded mock data for core scenarios (copied from previous main.py logic)
        """
        results = []
        images = []
        
        q_lower = query.lower()
        
        if "ë³´ê±´ì†Œ" in query or "health center" in q_lower:
            results = [
                {"title": "ë³´ê±´ì†Œ ì´ìš©ì•ˆë‚´ - G-Health ê³µê³µë³´ê±´í¬í„¸", "url": "https://www.g-health.kr/portal/index.do", "content": "ì „êµ­ ë³´ê±´ì†Œ ì°¾ê¸° ë° ì§„ë£Œ ì‹œê°„ ì•ˆë‚´. ë‚´ê³¼, ì¹˜ê³¼, í•œë°© ì§„ë£Œ ë“± ë³´ê±´ì†Œì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."},
                {"title": "ë³´ê±´ì†Œ - ì°¾ê¸°/ì•ˆë‚´/ì˜ˆì•½ - ì„œìš¸íŠ¹ë³„ì‹œ", "url": "https://health.seoul.go.kr", "content": "ì„œìš¸ì‹œ ë‚´ 25ê°œ ìì¹˜êµ¬ ë³´ê±´ì†Œ ìœ„ì¹˜ ë° ì—°ë½ì²˜ ì •ë³´. ì˜ˆë°©ì ‘ì¢…, ëŒ€ì‚¬ì¦í›„êµ° ê´€ë¦¬ ë“± ì‹œë¯¼ ê±´ê°• ì„œë¹„ìŠ¤ ì•ˆë‚´."},
                {"title": "ë™ë„¤ ì˜ì›ê³¼ ë³´ê±´ì†Œ, ë¬´ì—‡ì´ ë‹¤ë¥¼ê¹Œ? - í—¬ìŠ¤ì¡°ì„ ", "url": "https://m.health.chosun.com", "content": "ë³´ê±´ì†ŒëŠ” êµ­ê°€ì—ì„œ ìš´ì˜í•˜ëŠ” ê³µê³µ ì˜ë£Œê¸°ê´€ìœ¼ë¡œ, ì¼ë°˜ ë³‘ì˜ì›ë³´ë‹¤ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ì§„ë£Œ ë° ì˜ˆë°©ì ‘ì¢…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."},
                {"title": "ë³´ê±´ì†Œ ëª¨ë°”ì¼ í—¬ìŠ¤ì¼€ì–´ - í•œêµ­ê±´ê°•ì¦ì§„ê°œë°œì›", "url": "https://www.khealth.or.kr", "content": "ìŠ¤ë§ˆíŠ¸í°ì„ í™œìš©í•œ ë§ì¶¤í˜• ê±´ê°•ê´€ë¦¬ ì„œë¹„ìŠ¤. ë³´ê±´ì†Œ ì „ë¬¸ê°€ê°€ ë¹„ëŒ€ë©´ìœ¼ë¡œ ê±´ê°•ìƒë‹´ ë° ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."}
            ]
            images = [
                "https://news.seoul.go.kr/welfare/files/2020/02/602ff579e0a01.jpg",
                "https://www.korea.kr/newsWeb/resources/attaches/2021.05/20/094e9f735870ad46b412953258849646.jpg",
                "https://t1.daumcdn.net/cfile/tistory/99857B3359D8878D32",
                "https://www.yongin.go.kr/resources/images/hist/content/img_hist_2020_04_01.jpg"
            ]

        elif "ë‹¹ë‡¨" in query or "diabetes" in q_lower:
             results = [
                {"title": "ë‹¹ë‡¨ë³‘ì˜ ì¦ìƒê³¼ ì§„ë‹¨ - ëŒ€í•œë‹¹ë‡¨ë³‘í•™íšŒ", "url": "https://www.diabetes.or.kr", "content": "ëŒ€í‘œì ì¸ ì¦ìƒì€ ë‹¤ë‡¨, ë‹¤ìŒ, ë‹¤ì‹ì…ë‹ˆë‹¤. ì´ìœ  ì—†ëŠ” ì²´ì¤‘ ê°ì†Œë‚˜ í”¼ë¡œê°ë„ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤."},
                {"title": "ë‹¹ë‡¨ë³‘ ì´ˆê¸°ì¦ìƒ 5ê°€ì§€ - ì„œìš¸ì•„ì‚°ë³‘ì›", "url": "https://www.amc.seoul.kr", "content": "1. ì¦ì€ ì†Œë³€ 2. ì‹¬í•œ ê°ˆì¦ 3. ë°°ê³ í”” 4. ì²´ì¤‘ ê°ì†Œ 5. ì‹œì•¼ íë¦¼. ì¡°ê¸° ë°œê²¬ì´ í•©ë³‘ì¦ ì˜ˆë°©ì˜ í•µì‹¬ì…ë‹ˆë‹¤."},
                {"title": "ë‹¹ë‡¨ë³‘ ê´€ë¦¬ ê°€ì´ë“œ - ì§ˆë³‘ê´€ë¦¬ì²­", "url": "https://health.kdca.go.kr", "content": "ì•½ë¬¼ ìš”ë²• ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ì´ìš”ë²•ê³¼ ìš´ë™ìš”ë²•ì´ ë³‘í–‰ë˜ì–´ì•¼ í˜ˆë‹¹ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
             ]
             images = [
                 "https://www.amc.seoul.kr/asan/images/healthinfo/disease/disease_img_01.jpg",
                 "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000123&fileSn=1",
                 "https://i.ytimg.com/vi/example_diabetes/maxresdefault.jpg",
                 "https://post-phinf.pstatic.net/MjAyMTEyMTZfMjQ5/MDAxNjM5NjM4ODQ5MjQ5.example.jpg"
             ]
        
        elif "ê³ í˜ˆì••" in query or "hypertension" in q_lower:
            results = [
                {"title": "ê³ í˜ˆì••ì˜ ì§„ë‹¨ê³¼ ì¹˜ë£Œ - ì§ˆë³‘ê´€ë¦¬ì²­ êµ­ê°€ê±´ê°•ì •ë³´í¬í„¸", "url": "https://health.kdca.go.kr", "content": "ê³ í˜ˆì••ì€ ì¹¨ë¬µì˜ ì‚´ì¸ìë¡œ ë¶ˆë¦¬ë©°, ë‡Œì¡¸ì¤‘ ë° ì‹¬í˜ˆê´€ ì§ˆí™˜ì˜ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. ì •ê¸°ì ì¸ í˜ˆì•• ì¸¡ì •ê³¼ ìƒí™œ ìŠµê´€ ê°œì„ ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤."},
                {"title": "ëŒ€í•œê³ í˜ˆì••í•™íšŒ - ì¼ë°˜ì¸/í™˜ìë¥¼ ìœ„í•œ ì •ë³´", "url": "https://www.koreanhypertension.org", "content": "ì˜¬ë°”ë¥¸ í˜ˆì•• ì¸¡ì •ë²•, ê³ í˜ˆì•• ì•½ë¬¼ ë³µìš© ê°€ì´ë“œ, ì‹ë‹¨ ê´€ë¦¬ ë“± ê³ í˜ˆì•• í™˜ìë¥¼ ìœ„í•œ ì „ë¬¸ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"title": "ê³ í˜ˆì•• ë‚®ì¶”ëŠ” ë°©ë²• 5ê°€ì§€ - ì‚¼ì„±ì„œìš¸ë³‘ì› ê±´ê°•ì¹¼ëŸ¼", "url": "http://www.samsunghospital.com", "content": "1. ì²´ì¤‘ ê°ëŸ‰ 2. ì‹ë‹¨ ì¡°ì ˆ(ì €ì—¼ì‹) 3. ê·œì¹™ì ì¸ ìš´ë™ 4. ê¸ˆì—° 5. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬"},
                {"title": "ê³ í˜ˆì••, ì•½ í‰ìƒ ë¨¹ì–´ì•¼ í•˜ë‚˜ìš”?", "url": "https://www.hidoc.co.kr", "content": "ê³ í˜ˆì•• ì•½ì€ í•œ ë²ˆ ë¨¹ìœ¼ë©´ í‰ìƒ ë¨¹ì–´ì•¼ í•œë‹¤ëŠ” ì˜¤í•´ì™€ ì§„ì‹¤. ìƒí™œ ìŠµê´€ ê°œì„ ìœ¼ë¡œ í˜ˆì••ì´ ì¡°ì ˆë˜ë©´ ì•½ì„ ì¤„ì´ê±°ë‚˜ ëŠì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
            ]
            images = [
                "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000345&fileSn=1", 
                "https://i.ytimg.com/vi/Ofg98y0d_E4/maxresdefault.jpg", 
                "https://post-phinf.pstatic.net/MjAyMTAzMTZfMTQy/MDAxNjE1ODczMzE4MjU4.Kj-YlWfWlM_Zz3yW.jpg",
                "http://www.samsunghospital.com/upload/editor/20200518_1.jpg"
            ]
        
        elif "ê°ê¸°" in query or "cold" in q_lower:
            results = [
                 {"title": "ê°ê¸°ì™€ ë…ê°ì˜ ì°¨ì´ì  - ì§ˆë³‘ê´€ë¦¬ì²­", "url": "https://kdca.go.kr", "content": "ê°ê¸°ëŠ” ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì— ì˜í•œ ìƒê¸°ë„ ê°ì—¼ì´ë©°, ë…ê°ì€ ì¸í”Œë£¨ì—”ì ë°”ì´ëŸ¬ìŠ¤ì— ì˜í•œ ê¸‰ì„± í˜¸í¡ê¸° ì§ˆí™˜ì…ë‹ˆë‹¤."},
                 {"title": "ê°ê¸° ë¹¨ë¦¬ ë‚«ëŠ” ë²• 10ê°€ì§€", "url": "https://www.healthline.com", "content": "ì¶©ë¶„í•œ ìˆ˜ë¶„ ì„­ì·¨, íœ´ì‹, ê°€ìŠµê¸° ì‚¬ìš©, ë”°ëœ»í•œ ì°¨ ë§ˆì‹œê¸° ë“±ì´ ë„ì›€ì´ ë©ë‹ˆë‹¤."},
                 {"title": "ì•½ ë¨¹ì–´ë„ ê°ê¸°ê°€ ì•ˆ ë‚«ëŠ” ì´ìœ ", "url": "https://www.youtube.com/watch?v=example", "content": "ê°ê¸°ì•½ì€ ì¦ìƒì„ ì™„í™”í•  ë¿ ë°”ì´ëŸ¬ìŠ¤ë¥¼ ì¹˜ë£Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©´ì—­ë ¥ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."}
            ]
            images = [
                "https://img.freepik.com/free-photo/sick-woman-blowing-her-nose_23-2147743128.jpg",
                "https://i.ytimg.com/vi/example/maxresdefault.jpg",
                "https://images.unsplash.com/photo-1513201099718-4ed89549448f?ixlib=rb-1.2.1&auto=format&fit=crop&w=1000&q=80",
                "https://post-phinf.pstatic.net/MjAxOTEwMjlfMjQ4/MDAxNTcyMzE1MjE4MjQ4.example.jpg"
            ]

        return results, images
