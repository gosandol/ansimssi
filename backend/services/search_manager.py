import os
from tavily import TavilyClient
from .knowledge_base import KnowledgeBase

class SearchManager:
    def __init__(self):
        # API Keys
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        self.exa_key = os.getenv("EXA_API_KEY")
        self.brave_key = os.getenv("BRAVE_API_KEY")
        
        # Clients
        self.tavily_client = TavilyClient(api_key=self.tavily_key) if self.tavily_key else None
        self.knowledge_base = KnowledgeBase()

    def search_academic(self, query):
        """
        Search for academic papers using SerpApi (Google Scholar)
        """
        papers = []
        
        if self.serpapi_key:
            try:
                print(f"Attempting Academic Search (Scholar) for: {query}")
                import requests
                params = {
                    "engine": "google_scholar",
                    "q": query + " filetype:pdf",  # Prefer PDFs
                    "api_key": self.serpapi_key,
                    "num": 6,
                    "hl": "ko",
                    "as_ylo": "2020" # Recent papers (since 2020)
                }
                response = requests.get("https://serpapi.com/search", params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    organic_results = data.get("organic_results", [])
                    
                    papers = []
                    for item in organic_results:
                        # Logic to find the BEST link (PDF preferred)
                        # Scholar results often have 'resources': [{'title': 'so and so', 'link': '...pdf'}]
                        best_link = item.get("link")
                        pdf_link = None
                        
                        resources = item.get("resources", [])
                        for res in resources:
                            if "file_format" in res and "pdf" in res["file_format"].lower():
                                pdf_link = res.get("link")
                                break
                            if res.get("link", "").lower().endswith(".pdf"):
                                pdf_link = res.get("link")
                                break
                        
                        # If we found a direct PDF link, use it!
                        final_link = pdf_link if pdf_link else best_link
                        
                        # Extract Year cleanly
                        pub_info = item.get("publication_info", {}).get("summary", "")
                        year = ""
                        import re
                        match = re.search(r'\b20\d{2}\b', pub_info)
                        if match:
                            year = match.group(0)

                        papers.append({
                            "title": item.get("title"),
                            "link": final_link,
                            "snippet": item.get("snippet", ""),
                            "publication_info": pub_info,
                            "year": year
                        })
            except Exception as e:
                print(f"Academic Search Failed: {e}")
                
        # Mock Fallback if no results
        if not papers:
            print("Using Mock Academic Data")
            papers = [
                {
                    "title": f"A Study on {query} and its Clinical Implications",
                    "link": "#",
                    "snippet": f"This paper discusses the recent advancements in treating {query}...",
                    "publication_info": "Nature Medicine - 2025",
                    "year": "2025"
                },
                {
                    "title": f"Analysis of {query} in Korean Population",
                    "link": "#",
                    "snippet": "Comprehensive survey data regarding patient outcomes...",
                    "publication_info": "Journal of Korean Medical Science - 2024",
                    "year": "2024"
                }
            ]
            
        return papers

    async def search(self, query):
        """
        Execute Parallel Race Strategy (The "Gemini" Speed)
        """
        import asyncio
        
        results = []
        images = []
        source_engine = "none"

        # Define async wrappers for each provider
        async def run_google():
            if not self.serpapi_key: return None
            try:
                # SerpApi is blocking, so run in executor
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_google_sync, query)
            except Exception as e:
                print(f"Google Async Failed: {e}")
                return None

        async def run_tavily():
            if not self.tavily_client: return None
            try:
                # Tavily client is blocking
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_tavily_sync, query)
            except Exception as e:
                print(f"Tavily Async Failed: {e}")
                return None

        async def run_exa():
            if not self.exa_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_exa_sync, query)
            except Exception as e:
                print(f"Exa Async Failed: {e}")
                return None
                
        async def run_brave():
            if not self.brave_key: return None
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._search_brave_sync, query)
            except Exception as e:
                print(f"Brave Async Failed: {e}")
                return None

        # --- THE GREAT AGGREGATION (GEMINI STYLE) ---
        print(f"üß† Starting Deep Research (Aggregation) for: {query}")
        
        # Fire all requests simultaneously
        tasks = [
            asyncio.create_task(run_tavily()), # General Web & News
            asyncio.create_task(run_google()), # Real-time Sync & Local
            asyncio.create_task(run_exa()),    # Deep Content match
            # asyncio.create_task(run_brave())   # Backup (Skip to save quota/time if others sufficient)
        ]
        
        # Wait for ALL to complete (Enrichment Strategy)
        # VOICE-FIRST OPTIMIZATION: Adaptive Latency
        # 1. Primary Wait: 2.0s (Acceptable voice delay)
        done, pending = await asyncio.wait(tasks, timeout=2.0)
        
        aggregated_results = []
        seen_urls = set()
        
        # Check yield from first wave
        initial_yield = 0
        for task in done:
            try:
                res = task.result()
                if res and res.get('results'):
                    initial_yield += len(res['results'])
            except: pass
            
        # 2. Decision Gate: Do we have enough?
        # If we have < 4 results, it's too thin. Pay the latency cost for intelligence.
        # If we have >= 4, SPEED WINS.
        if initial_yield < 4 and len(pending) > 0:
            print(f"‚ö†Ô∏è Low yield ({initial_yield}) after 2s. Extending wait for deep research...")
            second_wave_done, second_wave_pending = await asyncio.wait(pending, timeout=2.0)
            
            # Meritge second wave into done
            done = done.union(second_wave_done)
            pending = second_wave_pending # Remainder are truly slow/dead
        else:
            print(f"‚ö°Ô∏è Voice Speed Success: {initial_yield} results in <2.0s. Proceeding.")
        
        # Collect results from all successful engines (merged from both waves)
        for task in done:
            try:
                res = task.result()
                if res and res.get('results'):
                    engine_name = res.get('engine')
                    print(f"‚úÖ {engine_name} contributed {len(res['results'])} results.")
                    
                    # Add images if available
                    if res.get('images'):
                        images.extend(res['images'])
                        
                    # Add unique results
                    for item in res['results']:
                        url = item.get('url')
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            # Tag the source engine for debugging/quality check
                            item['source_engine'] = engine_name 
                            aggregated_results.append(item)
            except Exception as e:
                print(f"Task Error during aggregation: {e}")
                
        # Cancel any stragglers (Too slow for voice)
        for t in pending: t.cancel()

        # Tier 5: Emergency Fallback if ABSOLUTELY nothing found
        if not aggregated_results:
            print("‚ö†Ô∏è No external results found. Entering Emergency Fallback...")
            kb_match = self.knowledge_base.find_match(query)
            if kb_match:
                print("Tier 5-A Success: KB")
                aggregated_results = kb_match.get('sources', [])
                images = kb_match.get('images', [])
                source_engine = "knowledge_base"
            else:
                print("Tier 5-B: Hardcoded Mock")
                aggregated_results, images = self._get_mock_data(query)
                source_engine = "mock"
        else:
            source_engine = "hybrid_aggregation"
            
        # Limit total context to avoid token overflow? 
        # For now, let's keep top 8-10 high quality ones.
        # Simple heuristic: Interleave results? 
        # Or just take top 10 from the mixed bag.
        final_results = aggregated_results[:12]
        
        print(f"üèÜ Final Aggregated Context: {len(final_results)} items.")

        return final_results, images, source_engine

    # --- Sync Helper Implementations ---
    def _search_google_sync(self, query):
        print(f"Attempting Tier 1 (Google) for: {query}")
        import requests
        params = {
            "engine": "google",
            "q": query,
            "api_key": self.serpapi_key,
            "num": 5,
            "hl": "ko", "gl": "kr"
        }
        response = requests.get("https://serpapi.com/search", params=params)
        if response.status_code == 200:
            data = response.json()
            organic = data.get("organic_results", [])
            results = [{"title": i.get("title"), "url": i.get("link"), "content": i.get("snippet", "")} for i in organic]
            if results: return {"engine": "google", "results": results, "images": []}
        return None

    def _search_tavily_sync(self, query):
        print(f"Attempting Tier 2 (Tavily) for: {query}")
        search_result = self.tavily_client.search(query=query, search_depth="basic", include_images=True)
        results = search_result.get("results", [])
        images = search_result.get("images", [])
        if results: return {"engine": "tavily", "results": results, "images": images}
        return None

    def _search_exa_sync(self, query):
        print(f"Attempting Tier 3 (Exa) for: {query}")
        import requests
        headers = {"accept": "application/json", "content-type": "application/json", "x-api-key": self.exa_key}
        response = requests.post("https://api.exa.ai/search", json={"query": query, "numResults": 5, "useAutoprompt": True, "contents": {"text": True}}, headers=headers)
        if response.status_code == 200:
            data = response.json()
            results = [{"title": i.get("title") or "Exa Result", "url": i.get("url"), "content": i.get("text", "")[:300] + "..."} for i in data.get("results", [])]
            if results: return {"engine": "exa", "results": results, "images": []}
        return None

    def _search_brave_sync(self, query):
         print(f"Attempting Tier 4 (Brave) for: {query}")
         import requests
         headers = {"Accept": "application/json", "X-Subscription-Token": self.brave_key}
         response = requests.get("https://api.search.brave.com/res/v1/web/search", params={"q": query, "count": 5}, headers=headers)
         if response.status_code == 200:
             data = response.json()
             results = [{"title": i.get("title"), "url": i.get("url"), "content": i.get("description")} for i in data.get("web", {}).get("results", [])]
             if results: return {"engine": "brave", "results": results, "images": []}
         return None

    def _get_mock_data(self, query):
        """
        Hardcoded mock data for core scenarios (copied from previous main.py logic)
        """
        results = []
        images = []
        
        q_lower = query.lower()
        
        if "Î≥¥Í±¥ÏÜå" in query or "health center" in q_lower:
            results = [
                {"title": "Î≥¥Í±¥ÏÜå Ïù¥Ïö©ÏïàÎÇ¥ - G-Health Í≥µÍ≥µÎ≥¥Í±¥Ìè¨ÌÑ∏", "url": "https://www.g-health.kr/portal/index.do", "content": "Ï†ÑÍµ≠ Î≥¥Í±¥ÏÜå Ï∞æÍ∏∞ Î∞è ÏßÑÎ£å ÏãúÍ∞Ñ ÏïàÎÇ¥. ÎÇ¥Í≥º, ÏπòÍ≥º, ÌïúÎ∞© ÏßÑÎ£å Îì± Î≥¥Í±¥ÏÜåÏóêÏÑú Ï†úÍ≥µÌïòÎäî Îã§ÏñëÌïú ÏùòÎ£å ÏÑúÎπÑÏä§Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî."},
                {"title": "Î≥¥Í±¥ÏÜå - Ï∞æÍ∏∞/ÏïàÎÇ¥/ÏòàÏïΩ - ÏÑúÏö∏ÌäπÎ≥ÑÏãú", "url": "https://health.seoul.go.kr", "content": "ÏÑúÏö∏Ïãú ÎÇ¥ 25Í∞ú ÏûêÏπòÍµ¨ Î≥¥Í±¥ÏÜå ÏúÑÏπò Î∞è Ïó∞ÎùΩÏ≤ò Ï†ïÎ≥¥. ÏòàÎ∞©Ï†ëÏ¢Ö, ÎåÄÏÇ¨Ï¶ùÌõÑÍµ∞ Í¥ÄÎ¶¨ Îì± ÏãúÎØº Í±¥Í∞ï ÏÑúÎπÑÏä§ ÏïàÎÇ¥."},
                {"title": "ÎèôÎÑ§ ÏùòÏõêÍ≥º Î≥¥Í±¥ÏÜå, Î¨¥ÏóáÏù¥ Îã§Î•ºÍπå? - Ìó¨Ïä§Ï°∞ÏÑ†", "url": "https://m.health.chosun.com", "content": "Î≥¥Í±¥ÏÜåÎäî Íµ≠Í∞ÄÏóêÏÑú Ïö¥ÏòÅÌïòÎäî Í≥µÍ≥µ ÏùòÎ£åÍ∏∞Í¥ÄÏúºÎ°ú, ÏùºÎ∞ò Î≥ëÏùòÏõêÎ≥¥Îã§ Ï†ÄÎ†¥Ìïú ÎπÑÏö©ÏúºÎ°ú ÏßÑÎ£å Î∞è ÏòàÎ∞©Ï†ëÏ¢ÖÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§."},
                {"title": "Î≥¥Í±¥ÏÜå Î™®Î∞îÏùº Ìó¨Ïä§ÏºÄÏñ¥ - ÌïúÍµ≠Í±¥Í∞ïÏ¶ùÏßÑÍ∞úÎ∞úÏõê", "url": "https://www.khealth.or.kr", "content": "Ïä§ÎßàÌä∏Ìè∞ÏùÑ ÌôúÏö©Ìïú ÎßûÏ∂§Ìòï Í±¥Í∞ïÍ¥ÄÎ¶¨ ÏÑúÎπÑÏä§. Î≥¥Í±¥ÏÜå Ï†ÑÎ¨∏Í∞ÄÍ∞Ä ÎπÑÎåÄÎ©¥ÏúºÎ°ú Í±¥Í∞ïÏÉÅÎã¥ Î∞è Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§."}
            ]
            images = [
                "https://news.seoul.go.kr/welfare/files/2020/02/602ff579e0a01.jpg",
                "https://www.korea.kr/newsWeb/resources/attaches/2021.05/20/094e9f735870ad46b412953258849646.jpg",
                "https://t1.daumcdn.net/cfile/tistory/99857B3359D8878D32",
                "https://www.yongin.go.kr/resources/images/hist/content/img_hist_2020_04_01.jpg"
            ]

        elif "ÎãπÎá®" in query or "diabetes" in q_lower:
             results = [
                {"title": "ÎãπÎá®Î≥ëÏùò Ï¶ùÏÉÅÍ≥º ÏßÑÎã® - ÎåÄÌïúÎãπÎá®Î≥ëÌïôÌöå", "url": "https://www.diabetes.or.kr", "content": "ÎåÄÌëúÏ†ÅÏù∏ Ï¶ùÏÉÅÏùÄ Îã§Îá®, Îã§Ïùå, Îã§ÏãùÏûÖÎãàÎã§. Ïù¥Ïú† ÏóÜÎäî Ï≤¥Ï§ë Í∞êÏÜåÎÇò ÌîºÎ°úÍ∞êÎèÑ ÎÇòÌÉÄÎÇ† Ïàò ÏûàÏäµÎãàÎã§."},
                {"title": "ÎãπÎá®Î≥ë Ï¥àÍ∏∞Ï¶ùÏÉÅ 5Í∞ÄÏßÄ - ÏÑúÏö∏ÏïÑÏÇ∞Î≥ëÏõê", "url": "https://www.amc.seoul.kr", "content": "1. Ïû¶ÏùÄ ÏÜåÎ≥Ä 2. Ïã¨Ìïú Í∞àÏ¶ù 3. Î∞∞Í≥†Ìîî 4. Ï≤¥Ï§ë Í∞êÏÜå 5. ÏãúÏïº ÌùêÎ¶º. Ï°∞Í∏∞ Î∞úÍ≤¨Ïù¥ Ìï©Î≥ëÏ¶ù ÏòàÎ∞©Ïùò ÌïµÏã¨ÏûÖÎãàÎã§."},
                {"title": "ÎãπÎá®Î≥ë Í¥ÄÎ¶¨ Í∞ÄÏù¥Îìú - ÏßàÎ≥ëÍ¥ÄÎ¶¨Ï≤≠", "url": "https://health.kdca.go.kr", "content": "ÏïΩÎ¨º ÏöîÎ≤ï ÎøêÎßå ÏïÑÎãàÎùº ÏãùÏù¥ÏöîÎ≤ïÍ≥º Ïö¥ÎèôÏöîÎ≤ïÏù¥ Î≥ëÌñâÎêòÏñ¥Ïïº ÌòàÎãπÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§."}
             ]
             images = [
                 "https://www.amc.seoul.kr/asan/images/healthinfo/disease/disease_img_01.jpg",
                 "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000123&fileSn=1",
                 "https://i.ytimg.com/vi/example_diabetes/maxresdefault.jpg",
                 "https://post-phinf.pstatic.net/MjAyMTEyMTZfMjQ5/MDAxNjM5NjM4ODQ5MjQ5.example.jpg"
             ]
        
        elif "Í≥†ÌòàÏïï" in query or "hypertension" in q_lower:
            results = [
                {"title": "Í≥†ÌòàÏïïÏùò ÏßÑÎã®Í≥º ÏπòÎ£å - ÏßàÎ≥ëÍ¥ÄÎ¶¨Ï≤≠ Íµ≠Í∞ÄÍ±¥Í∞ïÏ†ïÎ≥¥Ìè¨ÌÑ∏", "url": "https://health.kdca.go.kr", "content": "Í≥†ÌòàÏïïÏùÄ Ïπ®Î¨µÏùò ÏÇ¥Ïù∏ÏûêÎ°ú Î∂àÎ¶¨Î©∞, ÎáåÏ°∏Ï§ë Î∞è Ïã¨ÌòàÍ¥Ä ÏßàÌôòÏùò Ï£ºÏöî ÏõêÏù∏ÏûÖÎãàÎã§. Ï†ïÍ∏∞Ï†ÅÏù∏ ÌòàÏïï Ï∏°Ï†ïÍ≥º ÏÉùÌôú ÏäµÍ¥Ä Í∞úÏÑ†Ïù¥ ÌïÑÏàòÏ†ÅÏûÖÎãàÎã§."},
                {"title": "ÎåÄÌïúÍ≥†ÌòàÏïïÌïôÌöå - ÏùºÎ∞òÏù∏/ÌôòÏûêÎ•º ÏúÑÌïú Ï†ïÎ≥¥", "url": "https://www.koreanhypertension.org", "content": "Ïò¨Î∞îÎ•∏ ÌòàÏïï Ï∏°Ï†ïÎ≤ï, Í≥†ÌòàÏïï ÏïΩÎ¨º Î≥µÏö© Í∞ÄÏù¥Îìú, ÏãùÎã® Í¥ÄÎ¶¨ Îì± Í≥†ÌòàÏïï ÌôòÏûêÎ•º ÏúÑÌïú Ï†ÑÎ¨∏Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§."},
                {"title": "Í≥†ÌòàÏïï ÎÇÆÏ∂îÎäî Î∞©Î≤ï 5Í∞ÄÏßÄ - ÏÇºÏÑ±ÏÑúÏö∏Î≥ëÏõê Í±¥Í∞ïÏπºÎüº", "url": "http://www.samsunghospital.com", "content": "1. Ï≤¥Ï§ë Í∞êÎüâ 2. ÏãùÎã® Ï°∞Ï†à(Ï†ÄÏóºÏãù) 3. Í∑úÏπôÏ†ÅÏù∏ Ïö¥Îèô 4. Í∏àÏó∞ 5. Ïä§Ìä∏Î†àÏä§ Í¥ÄÎ¶¨"},
                {"title": "Í≥†ÌòàÏïï, ÏïΩ ÌèâÏÉù Î®πÏñ¥Ïïº ÌïòÎÇòÏöî?", "url": "https://www.hidoc.co.kr", "content": "Í≥†ÌòàÏïï ÏïΩÏùÄ Ìïú Î≤à Î®πÏúºÎ©¥ ÌèâÏÉù Î®πÏñ¥Ïïº ÌïúÎã§Îäî Ïò§Ìï¥ÏôÄ ÏßÑÏã§. ÏÉùÌôú ÏäµÍ¥Ä Í∞úÏÑ†ÏúºÎ°ú ÌòàÏïïÏù¥ Ï°∞Ï†àÎêòÎ©¥ ÏïΩÏùÑ Ï§ÑÏù¥Í±∞ÎÇò ÎÅäÏùÑ Ïàò ÏûàÏäµÎãàÎã§."}
            ]
            images = [
                "https://health.kdca.go.kr/healthinfo/biz/health/file/fileDownload.do?atchFileId=FILE_000000000000345&fileSn=1", 
                "https://i.ytimg.com/vi/Ofg98y0d_E4/maxresdefault.jpg", 
                "https://post-phinf.pstatic.net/MjAyMTAzMTZfMTQy/MDAxNjE1ODczMzE4MjU4.Kj-YlWfWlM_Zz3yW.jpg",
                "http://www.samsunghospital.com/upload/editor/20200518_1.jpg"
            ]
        
        elif "Í∞êÍ∏∞" in query or "cold" in q_lower:
            results = [
                 {"title": "Í∞êÍ∏∞ÏôÄ ÎèÖÍ∞êÏùò Ï∞®Ïù¥Ï†ê - ÏßàÎ≥ëÍ¥ÄÎ¶¨Ï≤≠", "url": "https://kdca.go.kr", "content": "Í∞êÍ∏∞Îäî Î∞îÏù¥Îü¨Ïä§ Í∞êÏóºÏóê ÏùòÌïú ÏÉÅÍ∏∞ÎèÑ Í∞êÏóºÏù¥Î©∞, ÎèÖÍ∞êÏùÄ Ïù∏ÌîåÎ£®ÏóîÏûê Î∞îÏù¥Îü¨Ïä§Ïóê ÏùòÌïú Í∏âÏÑ± Ìò∏Ìù°Í∏∞ ÏßàÌôòÏûÖÎãàÎã§."},
                 {"title": "Í∞êÍ∏∞ Îπ®Î¶¨ ÎÇ´Îäî Î≤ï 10Í∞ÄÏßÄ", "url": "https://www.healthline.com", "content": "Ï∂©Î∂ÑÌïú ÏàòÎ∂Ñ ÏÑ≠Ï∑®, Ìú¥Ïãù, Í∞ÄÏäµÍ∏∞ ÏÇ¨Ïö©, Îî∞ÎúªÌïú Ï∞® ÎßàÏãúÍ∏∞ Îì±Ïù¥ ÎèÑÏõÄÏù¥ Îê©ÎãàÎã§."},
                 {"title": "ÏïΩ Î®πÏñ¥ÎèÑ Í∞êÍ∏∞Í∞Ä Ïïà ÎÇ´Îäî Ïù¥Ïú†", "url": "https://www.youtube.com/watch?v=example", "content": "Í∞êÍ∏∞ÏïΩÏùÄ Ï¶ùÏÉÅÏùÑ ÏôÑÌôîÌï† Îøê Î∞îÏù¥Îü¨Ïä§Î•º ÏπòÎ£åÌïòÏßÄ ÏïäÏäµÎãàÎã§. Î©¥Ïó≠Î†•Ïù¥ Ï§ëÏöîÌï©ÎãàÎã§."}
            ]
            images = [
                "https://img.freepik.com/free-photo/sick-woman-blowing-her-nose_23-2147743128.jpg",
                "https://i.ytimg.com/vi/example/maxresdefault.jpg",
                "https://images.unsplash.com/photo-1513201099718-4ed89549448f?ixlib=rb-1.2.1&auto=format&fit=crop&w=1000&q=80",
                "https://post-phinf.pstatic.net/MjAxOTEwMjlfMjQ4/MDAxNTcyMzE1MjE4MjQ4.example.jpg"
            ]

        return results, images
